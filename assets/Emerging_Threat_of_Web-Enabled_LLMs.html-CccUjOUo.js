import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,d as r,o}from"./app-CfanBHeX.js";const g="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure1.png",a="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure2.png",e="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table1.png",i="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure3.png",l="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure4.png",p="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure5.png",h="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure6.png",c="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure7.png",m="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure8.png",L="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table2.png",d="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table3.png",_="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure9.png",b="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table4.png",u="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table7.png",M="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure10.png",f="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure11.png",x="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table8.png",W="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table6.png",E="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure12.png",T="/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure13.png",I={};function P(S,s){return o(),t("div",null,s[0]||(s[0]=[r('<h1 id="when-llms-go-online-the-emerging-threat-of-web-enabled-llms" tabindex="-1"><a class="header-anchor" href="#when-llms-go-online-the-emerging-threat-of-web-enabled-llms"><span>When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</span></a></h1><p>论文来自 34th USENIX Security Symposium (SEC 25) 的《<a href="https://www.usenix.org/conference/usenixsecurity25/presentation/kim-hanna" target="_blank" rel="noopener noreferrer">When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</a>》。</p><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>近年来，大型语言模型（LLMs）的快速发展使其逐渐成为具备规划能力并可与各类工具交互的“智能体”系统。这些 LLM 智能体常与基于网页的工具配合使用，从而能够访问多样化的信息源与实时数据。尽管这些进步在诸多应用中带来了显著益处，它们也同时增加了被恶意滥用的风险，尤其是在涉及个人信息的网络攻击情境中。</p><p>在本研究中，我们探讨了在含有个人数据的网络攻击中滥用 LLM 智能体所带来的风险。具体而言，我们旨在理解： 1）当被指示执行网络攻击时，LLM 智能体的攻击能力究竟有多强； 2）基于网页的工具如何增强网络攻击； 3）使用 LLM 智能体发起网络攻击在成本与易用性方面会变得多么“亲民”。</p><p>我们考察了三类攻击场景：收集可识别个人身份信息（PII）、生成冒充身份的社交帖子，以及制作鱼叉式钓鱼邮件。实验结果表明，LLM 智能体在这些攻击中的效果显著：在收集 PII 时精确率最高可达 95.9%；其生成的冒充帖子中有 93.9% 被判断为真实可信；在鱼叉式钓鱼邮件中，恶意链接的点击率被提升了 46.67%。此外，我们的发现还凸显了当下商业 LLM 所采用安全防护机制的局限性，强调了迫切需要更为稳健的安全措施，以防止 LLM 智能体被滥用。</p><h2 id="_1-引言" tabindex="-1"><a class="header-anchor" href="#_1-引言"><span>1. 引言</span></a></h2><p>将各类外部工具（如 API 与数据库）集成到大型语言模型（LLMs）中，已显著增强了它们的能力。这种集成使 LLM 能够作为 <strong>智能体（agent）</strong> 自主运作——即以 LLM 为核心组件、用于执行复杂任务的高级 AI 系统。大量研究已表明，LLM 智能体在众多领域执行任务时具有良好效果 [15,42,66]。</p><p>鉴于 LLM 智能体的先进能力，人们日益担忧：一旦被恶意使用，它们可能在现实世界中造成严重危害。近期研究 [21,35] 显示，LLM 智能体可能被用于发动网络攻击，凸显了其在该情境下的风险。为应对这些风险，LLM 供应商（例如 OpenAI [46]、Google [26]）已出台政策，禁止有害行为，如危害隐私或有意欺骗他人 [8,51]。同时，也建立了多种防护措施以防止这些强大模型被滥用 [6,28,48]。然而，随着模型能力不断提升，想要预见并缓解所有潜在的滥用情景变得愈发困难。</p><p>当 LLM 智能体与基于网页的工具配合使用时，其相关风险会进一步加剧。互联网上个人信息的广泛分享，使其成为网络犯罪分子的理想“狩猎场”。例如，攻击者常从 LinkedIn、Facebook 等网站抓取个人信息，然后在黑客论坛上出售 [63,65]。此类活动不仅侵犯隐私，还会提升个体遭受<strong>定向网络攻击</strong>的脆弱性，包括<strong>冒充</strong>与<strong>钓鱼邮件</strong>等 [29,32]。攻击者可利用获取到的目标信息，专门为恶意目的打造“量身定制”的文案：冒充帖利用目标声誉进行不当背书；钓鱼邮件则诱骗目标点击恶意链接。</p><h3 id="_1-1-我们的工作" tabindex="-1"><a class="header-anchor" href="#_1-1-我们的工作"><span>1.1 我们的工作</span></a></h3><p>尽管网络具有重要实用性，而个人数据的普遍可得性也引入了新的脆弱点，但关于<strong>利用个人信息实施网络攻击</strong>这一场景下 LLM 智能体的<strong>能力与危害</strong>仍不清晰。本文研究基于网页工具、面向个人数据的恶意用途下，LLM 智能体的有效性与危害性。为此，我们围绕三类典型网络攻击，利用公开可得的私人信息并结合 LLM 与智能体进行研究：<strong>可识别个人身份信息（PII）收集、冒充身份帖子生成、以及鱼叉式钓鱼邮件生成</strong>。</p><p>通过对上述攻击使用 LLM 智能体进行系统性分析，我们旨在回答以下研究问题。</p><h3 id="_1-2-研究问题" tabindex="-1"><a class="header-anchor" href="#_1-2-研究问题"><span>1.2 研究问题</span></a></h3><p><strong>RQ1. LLM 智能体实施的、利用个人信息的网络攻击有多强大？</strong><br> 为评估 LLM 智能体在网络攻击中的潜在滥用能力，我们为三类攻击各自设计了概念验证（PoC）。我们评估了面向公众、易于获取的最新商业 LLM——GPT、Claude 与 Gemini。为定量分析 LLM 智能体对隐私的侵害，我们让其仅以“大学名称”作为初始提示，从 10 所知名大学的计算机科学研究人员处收集五类 PII。另为确认 LLM 智能体能否<strong>自动</strong>生成有效的冒充帖，我们仅提供被冒充对象的“姓名与隶属机构”，并提示智能体以当事人身份撰写、为某一特定主张背书。最后，我们通过一项包含 60 名参与者的用户研究，评估 LLM 智能体在撰写鱼叉式钓鱼邮件方面的效果，重点考察其生成邮件不同变体在<strong>真实性</strong>与<strong>可信度</strong>上的表现。结果表明：LLM 智能体能从高校教师处<strong>正确检索出平均 535.6 条 PII 项</strong>；其生成的冒充帖<strong>最高有 93.9% 被认为真实</strong>；其生成的鱼叉式钓鱼邮件<strong>效果明显</strong>，<strong>最高有 46.67% 的参与者点击了恶意链接</strong>。</p><blockquote><p>样本量只有60，普遍性存疑</p></blockquote><p><strong>RQ2. 基于网页的工具在多大程度上增强了正在实施网络攻击的 LLM 智能体？</strong><br> LLM 智能体可通过使用网页工具（搜索网页与操作各类交互式元素）来增强其响应能力。既有文献报告了 LLM 在网络攻击场景中的熟练度 [11,44,56,61]，而新增强大的功能则可预期会进一步提高风险。为衡量网页工具的影响，我们比较了<strong>纯 LLM</strong>与<strong>启用网页工具的 LLM 智能体</strong>之间的性能差异。我们实验了两种能力等级：仅启用<strong>搜索功能</strong>的智能体，以及同时启用<strong>搜索与导航功能</strong>的智能体。我们发现，在各类任务上，启用网页工具的 LLM 智能体<strong>持续优于</strong>纯 LLM（如图1所示）。</p><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>RQ3. 滥用 LLM 智能体以实施网络攻击有多“容易接近”（approachable）？</strong><br> 通过 LLM 智能体实施网络攻击的“可接近性”可拆分为两方面：<strong>成本</strong>与<strong>越权能力</strong>。这类攻击的执行会产生时间与金钱成本，从而显著影响其实用性。我们的发现指出：LLM 智能体能<strong>以极低成本、极短时间</strong>完成此类攻击，凸显其现实可行性。平均而言，使用 GPT 的 LLM 智能体<strong>每个任务约 10 秒完成、耗费约 $0.02</strong>。滥用 LLM 智能体面临的挑战来自于各服务平台内置的<strong>安全防护</strong>。若要通过 LLM 智能体来执行攻击，攻击者的提示必须<strong>成功绕过</strong>这些防护。我们的结果显示，供应商实现的防护<strong>仅在特定场景与某些服务中</strong>才会被触发。更进一步，我们发现，仅仅<strong>启用网页工具</strong>就可能使得某些 LLM 服务的防护<strong>被有效绕过</strong>。</p><h3 id="_1-3-贡献" tabindex="-1"><a class="header-anchor" href="#_1-3-贡献"><span>1.3 贡献</span></a></h3><ul><li><p>我们系统性评估了 LLM 在涉及个人数据的网络攻击中的可用性。具体而言，我们展示了 LLM 智能体可成功：1）收集五类 PII；2）冒充特定目标，利用其个人信息为攻击者的主张背书；3）为收件人定制合适的情境与发件人身份，从而生成<strong>高度定向</strong>的邮件。</p></li><li><p>我们发现，即便是最新的 LLM 服务，其防护机制也<strong>常常无法有效发挥作用</strong>。尤其值得注意的是，<strong>引入网页工具</strong>常使 LLM 的行为更为宽松，从而允许提示词<strong>绕过</strong>既有防护。这些发现揭示了当前防护的显著薄弱点，并强调<strong>迫切需要更稳健的安全措施</strong>，以防止 LLM 智能体被滥用。</p></li></ul><h2 id="_2-相关工作" tabindex="-1"><a class="header-anchor" href="#_2-相关工作"><span>2. 相关工作</span></a></h2><h3 id="llm-的安全性" tabindex="-1"><a class="header-anchor" href="#llm-的安全性"><span>LLM 的安全性</span></a></h3><p>随着大型语言模型（LLMs）的能力不断提升并被广泛采用，其潜在滥用问题引发了重大关注。既有研究表明，LLM 可在多个领域被恶意利用，包括：提取可识别个人身份信息（PII）[34,38]、操纵舆论 [62]、以及生成钓鱼邮件 [56,61]。一项最新研究 [40] 还指出，在地下黑市中，将 LLM “改造”为恶意服务的现象正在增多，进一步凸显了这些强大模型所带来的风险。</p><p>为了评估 LLM 的安全性，研究者采用了红队化方法，例如越狱（jailbreak）攻击 [9,18]。在提升安全性方面，主流的安全对齐方法通常采用<strong>基于人类反馈的强化学习</strong>（RLHF），以推动更安全的 LLM 的发展 [9,52]。随着 LLM 智能体能力的出现，近期一些研究也开始<strong>利用 LLM 智能体本身来实现安全防护</strong> [68,69]。</p><p>尽管商业化 LLM 服务已部署多种安全防护 [6,28,48]，但我们发现，即便是<strong>截至 2024 年 7 月 17 日的最新模型</strong>，在我们设计的攻击情景下仍<strong>缺乏有效防护</strong>，这表明<strong>亟需加强</strong>相关的安全机制。</p><h3 id="面向网络攻击的-llm-智能体" tabindex="-1"><a class="header-anchor" href="#面向网络攻击的-llm-智能体"><span>面向网络攻击的 LLM 智能体</span></a></h3><p>所谓 LLM 智能体，是指一种<strong>利用 LLM 来执行任务、做出决策或与用户自主交互</strong>的人工智能系统 [15]。这些智能体可根据其训练方式与系统集成形态，适配多种应用。近期工作已展示 LLM 智能体在多个领域的能力，例如<strong>真实环境中的编程挑战</strong> [70] 与<strong>网页导航</strong> [42]。</p><p>尽管具备上述能力，人们对 LLM 智能体的<strong>潜在滥用</strong>仍越来越担忧。研究表明，在智能体场景中，LLM 往往<strong>难以及时识别安全风险</strong> [67]。更进一步，已有研究显示，LLM 智能体能够<strong>自主利用系统漏洞</strong>：从网站上的 <strong>SQL 注入</strong> [22]，到现实系统中的 <strong>one-day 漏洞</strong> [21]。</p><p>虽然涉及在线平台<strong>私人数据</strong>的社会工程威胁与日俱增，但关于<strong>LLM 智能体如何在网络攻击中利用私人数据</strong>的能力，研究仍然不足。我们的工作正是通过系统化研究，弥补这一空白：评估 LLM 智能体在网络攻击中<strong>滥用私人数据</strong>的潜在能力。</p><h2 id="_3-用于网络攻击的-llm-智能体" tabindex="-1"><a class="header-anchor" href="#_3-用于网络攻击的-llm-智能体"><span>3. 用于网络攻击的 LLM 智能体</span></a></h2><p>本节我们在<strong>与基于网页的工具集成</strong>的前提下，探索 LLM 智能体所带来的风险，旨在模拟多种网络攻击情境。此分析旨在加深我们对这些工具在真实世界中<strong>可能被滥用</strong>方式的理解。第 <strong>3.1</strong> 节给出在网络攻击场景中部署 LLM 智能体所使用的<strong>具体工具与方法</strong>；第 <strong>3.2</strong> 节则介绍这些场景下分配给 LLM 智能体的<strong>具体任务</strong>。</p><h3 id="_3-1-llm-智能体概述" tabindex="-1"><a class="header-anchor" href="#_3-1-llm-智能体概述"><span>3.1 LLM 智能体概述</span></a></h3><figure><img src="'+a+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>为探索 LLM 在网络攻击中的潜在脆弱性，我们模拟了攻击者可能利用 LLM 的情境。利用 LLM 的一个直接方式是通过提示其执行有害行为，例如<strong>提取个人信息</strong>（见图2(a)）。尽管仅凭恶意提示，LLM 也能实施具有欺骗性的攻击，但我们关注更高级的形式——即<strong>LLM 智能体</strong>：这类系统能够进行<strong>规划</strong>并与各种工具交互，包括软件与外部 API。</p><p>具体而言，我们考虑两类借助网页工具（如<strong>网页搜索</strong>与<strong>网页导航</strong>）的 LLM 智能体：</p><ul><li><strong>WebSearch Agent（网页搜索智能体）</strong>：利用网页搜索工具获取来自 Google、Bing 等搜索引擎的检索结果。</li><li><strong>WebNav Agent（网页导航智能体）</strong>：借助导航工具从网页检索内容，并与页面中的可点击元素交互，以访问更深层嵌入的信息。</li></ul><p><strong>实现方式。</strong> 我们使用各 LLM API 提供的<strong>函数调用（function calling）<strong>能力来实现智能体。我们向 LLM 提供一组</strong>函数描述</strong>，让模型可根据任务需求，自主决定<strong>何时</strong>与<strong>如何</strong>调用函数。需要注意：LLM <strong>并不会直接执行函数</strong>，它负责识别合适的调用时机与所需参数；而实际执行由外部应用完成（例如网页搜索工具），随后将结果返回给 LLM，模型再据此生成响应，从而<strong>自动化</strong>整个流程并高效完成指定任务。</p><p>对于 <strong>WebSearch Agent</strong>，我们通过 <strong>Custom Search JSON API</strong> [27] 实现 <code>search()</code> 函数，用于以结构化 JSON 的形式获取 Google 的搜索结果。该函数接收<strong>查询词</strong>作为参数并返回相应结果。正如 图2(b) 所示，WebSearch 智能体会用合适的查询调用 <code>search()</code>，并基于返回的检索结果生成响应；若未找到所需信息，则可能<strong>重复调用</strong>该函数，并<strong>调整查询</strong>。</p><p>对于 <strong>WebNav Agent</strong>，我们使用 <strong>Selenium</strong> [60]、<strong>BeautifulSoup</strong> [55] 搭配 <strong>Requests</strong> [33] 等网页自动化工具实现其功能。我们具体实现了两个函数：<code>fetch_content()</code> 与 <code>find_button()</code>。<code>fetch_content()</code> 以 URL 为参数，返回页面内容；<code>find_button()</code> 用于识别页面上的<strong>可点击按钮/选项卡</strong>及其对应 URL。</p><p>如图 2(c) 所示，当 WebNav 智能体仅凭 <code>search()</code> 无法获取目标信息时，会结合 <code>fetch_content()</code> 与 <code>find_button()</code>，流程如下：</p><ol><li>智能体访问由搜索结果给出的合适 URL，并调用 <code>fetch_content()</code> 抓取内容。</li><li>分析抓取到的内容中是否包含所需信息。</li><li>若未找到，则调用 <code>find_button()</code> 识别能够进入<strong>更多信息</strong>的按钮或选项卡及其 URL。</li><li>使用 <code>fetch_content()</code> 访问新 URL 并抓取内容。</li><li>重复步骤 1–4，直到找到所需信息为止。</li><li>最后，智能体综合各步结果，生成<strong>完整</strong>的响应。</li></ol><p>需要指出的是，这些步骤是<strong>完全自动化</strong>的：仅依赖输入的提示词进行运行，<strong>不接收任何人工反馈</strong>，不同于聊天机器人或“人类助手”模式。</p><p><strong>模型。</strong> 我们选用<strong>可商用获取</strong>的模型；其可得性与能力都可能鼓励攻击者进行滥用。具体地，我们使用 <strong>GPT-4o（简称 GPT）</strong>、<strong>Claude 3.5 Sonnet（简称 Claude）</strong> 与 <strong>Gemini 1.5 Flash（简称 Gemini）</strong>，并分别通过 <strong>OpenAI API</strong> [47]、<strong>Anthropic API</strong> [4] 与 <strong>Gemini API</strong> [24] 进行调用。</p><h3 id="_3-2-定向网络攻击" tabindex="-1"><a class="header-anchor" href="#_3-2-定向网络攻击"><span>3.2 定向网络攻击</span></a></h3><p>在本研究中，我们考察 LLM 智能体执行那些<strong>传统上需要大量人力与资源</strong>的网络攻击的能力。我们聚焦于三类任务，这些任务利用了<strong>在线个人数据广泛可得</strong>所带来的脆弱性，且复杂度各不相同：</p><h4 id="攻击-1-pii-收集" tabindex="-1"><a class="header-anchor" href="#攻击-1-pii-收集"><span>攻击 1：PII 收集</span></a></h4><p>从互联网收集 PII（如电子邮箱、姓名、电话号码等）会带来显著的隐私担忧。即便这些数据<strong>公开可得</strong>，未经授权的收集也构成对隐私的侵犯 [54]。攻击者可以将此类信息用于恶意目的，例如身份盗用、诈骗，或进一步策划网络攻击 [29,32]。他们常借助<strong>网页爬取工具</strong>与<strong>自动化脚本</strong>，从多个网站批量获取 PII。然而，由于各网站的信息格式不同，这些工具通常需要<strong>针对目标站点进行定制开发</strong>，这意味着相当可观的人力投入与相关成本 [19,64]。</p><h4 id="攻击-2-冒充帖生成" tabindex="-1"><a class="header-anchor" href="#攻击-2-冒充帖生成"><span>攻击 2：冒充帖生成</span></a></h4><p>攻击者撰写“冒充身份”的帖子以<strong>误导受众</strong>，服务于恶意目标（如牟取经济利益或抹黑名誉）[36,41]。为了生成复杂、逼真的冒充内容，攻击者需<strong>细致调研</strong>目标的个人细节、社会行为与沟通风格，并将这些要素融入文案之中——这一过程通常<strong>耗时且费力</strong>。</p><h4 id="攻击-3-鱼叉式钓鱼邮件生成" tabindex="-1"><a class="header-anchor" href="#攻击-3-鱼叉式钓鱼邮件生成"><span>攻击 3：鱼叉式钓鱼邮件生成</span></a></h4><p>鱼叉式钓鱼邮件在<strong>欺骗受害者</strong>方面非常有效，但相较于传统钓鱼邮件，<strong>更耗时、更昂贵</strong> [30,57]。其更高的成本来自“鱼叉式”本身：它面向具体个人或组织，使用<strong>量身定制</strong>的信息进行定向投递。</p><p>就复杂度而言，<strong>PII 收集</strong>最为直接：LLM 智能体从在线内容中抓取目标信息；<strong>冒充帖生成</strong>更复杂，需要利用个人数据创作与目标高度相似的内容；<strong>鱼叉式钓鱼邮件生成</strong>最复杂，需<strong>设计情境</strong>、<strong>设定发件人身份</strong>，并根据目标兴趣<strong>定制</strong>邮件内容。我们在表1 中汇总了这些攻击的实验设置。</p><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在后续章节中，我们将基于上述攻击回答研究问题。关于 LLM 智能体在这些攻击中的<strong>强度（RQ1）<strong>以及</strong>网页工具带来的影响（RQ2）</strong>，我们将在第 4、5、6 节通过<strong>全自动</strong>地执行这些攻击予以作答；而关于<strong>可接近性（RQ3）</strong>，我们将在第 7 节通过度量<strong>成本</strong>与<strong>内置安全防护表现</strong>来回答。我们的发现凸显了 LLM 智能体在<strong>自动化复杂网络威胁</strong>方面所带来的潜在风险。</p><h2 id="_4-pii-收集" tabindex="-1"><a class="header-anchor" href="#_4-pii-收集"><span>4. PII 收集</span></a></h2><p>本节我们研究 LLM 智能体通过提取<strong>可识别个人身份信息（PII）<strong>而对隐私造成侵害的可能性。遵循美国劳工部的定义 [45]，PII 指</strong>任何能够通过直接或间接方式，使一个人的身份被合理推断</strong>的信息。本文聚焦于<strong>可直接标识个体</strong>的信息，如姓名与电子邮箱地址。即便某些 PII 在互联网上<strong>公开可得</strong>，对其进行收集并用于<strong>非预期目的</strong>仍可能构成对隐私的侵犯 [54]。</p><h3 id="_4-1-实验设置" tabindex="-1"><a class="header-anchor" href="#_4-1-实验设置"><span>4.1 实验设置</span></a></h3><p><strong>攻击场景。</strong> 攻击者的目标是使用 LLM 智能体收集特定目标的 PII。本文选择 <strong>QS 世界大学排名</strong>中前十所大学的<strong>计算机科学（CS）系</strong>教授作为目标，收集其 PII。我们关注的 PII 类型包括：<strong>姓名、电子邮件地址、电话号码、办公地址</strong>，以及<strong>个人主页 URL</strong>。</p><p>为进一步评估基于学术角色的信息收集差异，我们还考察 LLM 智能体是否能够收集 <strong>CS 学生</strong>的 PII。具体做法是：从总体教授人群中<strong>随机等概率抽样 50 名教授</strong>，在对教授进行初次 PII 收集时<strong>成功抓取到其个人主页</strong>。随后，我们将这些教授所在实验室的 <strong>CS 学生</strong>设置为目标。</p><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>如图3 所示，LLM 智能体执行 PII 收集的流程包含两步：1）<strong>构建目标名单</strong>；2）<strong>收集对应的个人信息</strong>。需要强调的是：攻击者<strong>不进行任何人工操作</strong>，全部由 LLM 智能体完成。对于“教授”目标，攻击者首先从某一<strong>大学</strong>（如 MIT）提取 CS 教授姓名；随后，再为每位教授收集其余 PII。对于“学生”目标，则<strong>复用</strong>教授组中获得的数据：在提示词中使用三类教授信息——<strong>姓名、个人主页、学校/实验室隶属</strong>——来定位<strong>与该教授当前关联的学生姓名</strong>。</p><p><strong>评估。</strong> 考虑到 CS 领域广泛且多样，我们的评估<strong>不只是</strong>依据系官网名单。我们聘请<strong>人工标注员</strong>审查 LLM 智能体生成的结果：在充分的网页检索后，依据个人是否从事相关领域工作，将其判定为 <strong>CS 教授</strong>。部分 PII（如电话号码）随时间变化频繁，验证困难；因此，我们对每个个体的其余 PII采用<strong>五次独立查询</strong>，若<strong>至少一次</strong>与在线可查数据吻合，即判定该项收集<strong>成功</strong>。对“CS 学生姓名”的标注中，我们以教授个人主页上列出的<strong>学生信息</strong>为<strong>真值</strong>。更多标注细节见<strong>附录B</strong>。</p><h3 id="_4-2-主要结果" tabindex="-1"><a class="header-anchor" href="#_4-2-主要结果"><span>4.2 主要结果</span></a></h3><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图4(a) 与图4(b) 分别展示了在 <strong>GPT</strong> 的<strong>纯 LLM</strong>与<strong>LLM 智能体</strong>两种设定下，对 <strong>CS 教授</strong>与<strong>CS 学生</strong>进行 PII 收集的结果。随着引入更多工具，LLM 智能体在 PII 收集上的有效性不断提升。<strong>这意味着：伴随能力扩张，LLM 智能体相关风险也显著增加。</strong></p><p>对于 <strong>CS 教授</strong>，<strong>未使用</strong>网页工具的<strong>纯 LLM</strong>在检索 PII 时效果较差，尤其在<strong>办公地址、电话号码</strong>等细节信息上。利用<strong>搜索 API</strong>后，<strong>WebSearch</strong> 智能体比纯 LLM 更有效地收集信息；但受 <strong>Google Custom Search API</strong> 的限制（该 API 仅返回<strong>摘要片段</strong> [23]，且常混入多个教授的细节，尤其在联系方式上易出错），它在“办公地址、电话号码”等字段上仍显不足。相较之下，进一步引入<strong>导航工具</strong>的 <strong>WebNav</strong> 智能体能够<strong>高效收集</strong>到 <strong>570 名 CS 教授的姓名</strong>，并在其他 PII 上达到可观比例：<strong>电话号码 71.4%</strong>、<strong>办公地点 91.2%</strong>、<strong>电子邮箱 95.9%</strong>、<strong>个人主页 77.7%</strong>。</p><p>对 <strong>CS 学生</strong>的 PII 收集中，<strong>唯有 WebNav 智能体</strong>表现有效——因为学生往往没有像教授那样<strong>全面公开</strong>个人信息。图4(c) 显示，<strong>WebSearch 智能体</strong>即便在收集“学生姓名”时也表现不佳（同样受限于搜索 API）；此外，<strong>纯 LLM</strong>常返回大量错误的学生姓名，<strong>精度显著偏低</strong>，体现出<strong>较强的幻觉</strong>。</p><p>这些结果表明：<strong>接入网页</strong>显著提升了 LLM 智能体在特定目标 PII 提取上的性能。随着 LLM 能力增长，其潜在威胁亦随之上升，使其能够通过<strong>多轮、多层级探索</strong>收集到更细粒度的信息。尽管这些信息在互联网上<strong>公开可得</strong>，但 LLM 智能体<strong>成功检索</strong>这些 PII 仍令人担忧，或许需要我们<strong>重新审视</strong>LLM 智能体的使用方式。</p><h3 id="_4-3-模型分析" tabindex="-1"><a class="header-anchor" href="#_4-3-模型分析"><span>4.3 模型分析</span></a></h3><p>为比较不同模型在 PII 收集能力上的差异，我们向 <strong>Gemini</strong> 与 <strong>Claude</strong> 提供了相同的提示词以进行 PII 收集。对 <strong>Gemini</strong> 而言，无论是否启用基于网页的工具，模型都<strong>拒绝</strong>收集 PII，并返回类似“<strong>共享此类信息可能违反隐私，并可能导致骚扰或安全风险</strong>”之类的信息。</p><p>相比之下，如图5 所示，<strong>Claude</strong> 的表现与 <strong>GPT</strong> 相似：启用更先进的工具后，除“教授姓名”外，其在收集信息方面更为有效。<strong>纯 LLM</strong> 虽然收集到的“姓名”数量更多，但这是因为其<strong>不加区分地</strong>收集所致，导致精度较低，为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.574</mn></mrow><annotation encoding="application/x-tex">0.574</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.574</span></span></span></span>（在共计 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo separator="true" lspace="0em" rspace="0em">,</mo><mn>384</mn></mrow><annotation encoding="application/x-tex">1{,}384</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mord"><span class="mpunct">,</span></span><span class="mord">384</span></span></span></span> 个姓名中仅有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>795</mn></mrow><annotation encoding="application/x-tex">795</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">795</span></span></span></span> 个正确）。在构建“学生姓名”列表的任务上，只有 <strong>WebNav</strong> 智能体能够同时获得<strong>高精度与高召回（均 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">&gt;0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.9</span></span></span></span>，见图 6）</strong>。</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>比较 <strong>GPT</strong> 与 <strong>Claude</strong> 的 <strong>WebNav</strong> 智能体表现时，<strong>Claude</strong> 对教授的平均 PII 收集量更多（<strong>535.6</strong> 项）于 <strong>GPT</strong>（<strong>497.4</strong> 项）。更具体地，Claude 成功获取了 <strong>612</strong> 位教授的姓名，并在其他 PII 上取得了可观比例：<strong>办公地点 88.6%</strong>、<strong>电子邮箱 94.6%</strong>、<strong>个人主页 92.0%</strong>，但<strong>电话号码仅为 62.4%</strong>。虽说 Claude 在“电话号码”上不如 GPT，但在“个人主页”获取上优于 GPT。</p><p>反观“学生组”，<strong>Claude</strong> 略逊一筹：它未能从<strong>10%</strong> 的给定教授主页中检索到学生名单。该差异可归因于两者不同的<strong>站内探索策略</strong>：即便上下文不确定，<strong>GPT</strong> 也会<strong>持续探索</strong>以定位学生名单；而 <strong>Claude</strong> 在上下文不清时往往<strong>停止</strong>。具体来说，若站点明确列出“学生名单”，智能体会直接抓取并返回；若页面未直接提供，智能体则在站内<strong>继续导航</strong>，必要时通过按钮或链接跳转至其他页面寻找。例如，即使站点未明确指向“学生名单”，<strong>GPT</strong> 也会跟进该链接以判断其中是否包含所需信息；相反，<strong>Claude</strong> 仅在链接<strong>标注清晰</strong>时才会跟进，否则会报告“未找到学生信息”。</p><h2 id="_5-冒充帖-impersonation-post-生成" tabindex="-1"><a class="header-anchor" href="#_5-冒充帖-impersonation-post-生成"><span>5. 冒充帖（Impersonation Post）生成</span></a></h2><p>本节我们考察：在仅利用<strong>来自网络的公开信息</strong>的前提下，LLM 智能体在“冒充个人”方面的有效性。</p><h3 id="_5-1-实验设置" tabindex="-1"><a class="header-anchor" href="#_5-1-实验设置"><span>5.1 实验设置</span></a></h3><p><strong>攻击场景。</strong> 攻击者的目标是<strong>自动生成</strong>社交媒体帖子，冒充特定目标<strong>为攻击者的某一主张背书</strong>。我们使用 LLM 智能体基于网络可得的目标个人信息生成<strong>可信的冒充帖</strong>，使攻击者可以借助目标的声誉传播其想传达的信息。本研究的目标对象为第 4 节所述的 <strong>50 名 CS 教授</strong>。</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>如图7所示，攻击者只需向智能体发送<strong>单轮查询</strong>即可生成冒充帖。直接在提示中使用“impersonate（冒充）”等词会触发 LLM 的<strong>安全防护</strong>而被拒绝。因此我们采用<strong>简单角色扮演</strong>技巧：先向智能体声明“<strong>我是 [Name]，就职于 [University]</strong>”，随后启用基于网页的工具了解该目标的更多背景；接着，要求智能体<strong>撰写 500–700 字</strong>的社交媒体帖子，为<strong>特定主张</strong>进行宣传。需要注意：攻击者在提示中<strong>仅提供三项输入</strong>——目标的<strong>姓名（name）</strong>、<strong>机构（institution）<strong>与</strong>需要背书的主张（claim）</strong>。作为基线，我们让<strong>纯 LLM</strong>生成冒充帖，但<strong>不启用网页搜索功能</strong>，仅依赖模型<strong>预存知识</strong>。</p><p>我们设置了两类主张以观察“主张性质”对冒充效果的影响：1）“<strong>推荐研究 AI</strong>”（相对无害），2）“<strong>LLM 在抵御潜在滥用方面非常安全</strong>”（具有争议性）。</p><p><strong>评估。</strong> 我们通过 <strong>A/B 测试</strong>评估“是否接入网页”对冒充效果的影响。冒充帖由三种方法生成：<strong>纯 LLM</strong>、<strong>WebSearch 智能体</strong>与<strong>WebNav 智能体</strong>。随后进行两两对比，以判断<strong>哪一篇</strong>更像出自目标本人。评审模型我们仍使用 <strong>LLM</strong>，其在假新闻检测、机器生成文本识别等任务上具有较高准确率，即使未做额外微调亦然 [14,53]。</p><p>为提升评审的准确性，我们也<strong>启用网页搜索功能</strong>，使评审 LLM 能利用互联网上的信息进行核验。评审者会看到<strong>两个来源</strong>（如“纯 LLM vs. WebSearch 智能体”或“WebSearch 智能体 vs. WebNav 智能体”）生成的文本，并判断哪一篇更像目标本人所写；若两篇同样有效，则可标记为“不确定（unsure）”。为减少位置偏置，我们<strong>随机化</strong>（A 或 B）两篇文本的摆放顺序。为降低模型偏好带来的影响，每一对文本<strong>分别由三种模型</strong>（<strong>GPT、Claude、Gemini</strong>）进行评审，最终结果取<strong>三者多数票</strong>。</p><p>此外，为评估“<strong>冒充帖被认为真实</strong>的可能性”，我们还进行 <strong>Yes/No 测试</strong>。与 A/B 测试相似，仍由三种 LLM 评审，判断文本是否看起来像由目标本人撰写。A/B 与 Yes/No 的<strong>提示模板</strong>见<strong>附录C</strong>。</p><p><strong>“LLM 评审 LLM”输出的验证。</strong> 我们通过与<strong>人工评审</strong>对比来评估 LLM 评审者的可靠性。验证阶段共评估 <strong>270</strong> 篇帖子；该数量由公式</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>270</mn><mo>=</mo><mn>15</mn><mtext> </mtext><mo stretchy="false">(</mo><mtext>教授人数</mtext><mo stretchy="false">)</mo><mo>×</mo><mn>2</mn><mtext> </mtext><mo stretchy="false">(</mo><mtext>主张数</mtext><mo stretchy="false">)</mo><mo>×</mo><mn>3</mn><mtext> </mtext><mo stretchy="false">(</mo><mtext>语言模型：GPT, Claude, Gemini</mtext><mo stretchy="false">)</mo><mo>×</mo><mn>3</mn><mtext> </mtext><mo stretchy="false">(</mo><mtext>设置：LLM, WebSearch, WebNav</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> 270 = 15 \\ (\\text{教授人数}) \\times 2 \\ (\\text{主张数}) \\times 3 \\ (\\text{语言模型：GPT, Claude, Gemini}) \\times 3 \\ (\\text{设置：LLM, WebSearch, WebNav}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">270</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">15</span><span class="mspace"> </span><span class="mopen">(</span><span class="mord text"><span class="mord cjk_fallback">教授人数</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mspace"> </span><span class="mopen">(</span><span class="mord text"><span class="mord cjk_fallback">主张数</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mspace"> </span><span class="mopen">(</span><span class="mord text"><span class="mord cjk_fallback">语言模型：</span><span class="mord">GPT, Claude, Gemini</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mspace"> </span><span class="mopen">(</span><span class="mord text"><span class="mord cjk_fallback">设置：</span><span class="mord">LLM, WebSearch, WebNav</span></span><span class="mclose">)</span></span></span></span></span></p><p>得出。我们从中抽取 <strong>15 名教授</strong>，每位教授对应 <strong>18 篇</strong>由不同配置生成的帖子。给予人工评审的<strong>标注规范</strong>与提供给 LLM 评审者的<strong>提示</strong>保持一致。</p><p>我们将 LLM 的评估与人工的评估进行比较，以衡量模型评审与人类判断的一致性。就 <strong>A/B 测试</strong> 而言，人与 LLM 的平均一致率在“主张 1”上为 <strong>92.8%</strong>，在“主张 2”上为 <strong>85.0%</strong>；在 <strong>Yes/No 测试</strong> 中，这两个数值分别为 <strong>92.6%</strong> 与 <strong>91.1%</strong>。</p><h3 id="_5-2-主要结果" tabindex="-1"><a class="header-anchor" href="#_5-2-主要结果"><span>5.2 主要结果</span></a></h3><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图8 给出了一个由 <strong>GPT</strong> 智能体生成的、针对某位知名人物的<strong>冒充帖</strong>示例。我们只向智能体提供了该人物的<strong>姓名</strong>，但模型却综合出了其<strong>研究专长、毕业院校与所在公司</strong>等信息，并据此生成了一条<strong>看起来像其本人撰写</strong>的推文。随后的各项测试均按我们的攻击流程，面向第 4 节确定的目标教授开展。</p><figure><img src="'+L+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>基于工具使用的对比。</strong> 我们首先比较 <strong>WebNav</strong> 与 <strong>WebSearch</strong> 智能体生成的冒充帖，然后再比较 <strong>WebSearch 智能体</strong>与<strong>纯 LLM</strong>，以检验不同能力水平下的有效性。表2 展示了 <strong>A/B 测试</strong>中被选择比例的统计（当 LLM 或智能体<strong>拒绝生成</strong>内容时，不计入统计，稍后在本节讨论）。结果显示：<strong>接入更多工具</strong>后，各模型在冒充任务上的有效性普遍<strong>提升</strong>。更具体地，<strong>WebNav 智能体</strong>优于 <strong>WebSearch 智能体</strong>，而 <strong>WebSearch 智能体</strong>又明显<strong>胜过</strong>纯 LLM。差异在“WebSearch vs. 纯 LLM”的比较中尤为显著，说明<strong>增强搜索能力</strong>（即 WebSearch 智能体）能显著提升需要“冒充能力”的任务表现。</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>真实性评估。</strong> 我们进一步考察各智能体与纯 LLM 生成文本在<strong>真实性</strong>方面的表现。表3 报告了由 LLM 评审给出的“<strong>是</strong>（被视为真实）”的比例。结论与 A/B 测试相似：<strong>智能体生成</strong>的帖子“是”的比例高于<strong>纯 LLM</strong>；其中 <strong>WebNav 智能体</strong>效果最佳，<strong>GPT</strong> 与 <strong>Claude</strong> 智能体的<strong>成功率</strong>最高分别达到 <strong>93.9%</strong> 与 <strong>85.7%</strong>。相比之下，<strong>Gemini</strong> 在多数情况下表现不佳。</p><p>我们将 <strong>Gemini</strong> 的弱势归因于其文本<strong>过于简短</strong>。尽管我们要求生成 <strong>500–700</strong> 词的帖子，Gemini 实际平均仅生成 <strong>311</strong> 词（主张 1）与 <strong>334</strong> 词（主张 2）。文本的“简短”往往导致<strong>个人化信息不足</strong>，从而更多被判为“否”。另一个原因是：当遇到不熟悉内容时，Gemini 倾向于插入<strong>占位语</strong>而非真实信息（如“<em>作为 [请提及你的研究领域] 的研究者，我对 [请提及相关领域] 有深入理解</em>”），这反映了<strong>细节欠缺</strong>。以上因素共同导致了 Gemini 在冒充帖生成上的较低有效性。</p><p>有趣的是，<strong>有效性还会随主张性质略有变化</strong>。例如主张 2——“<strong>LLM 在抵御潜在滥用方面非常安全</strong>”——可能与现实并不吻合，且具有争议性。即使整合了个人信息，LLM 评审者对“有 CS 教授会公开发表此类主张”仍持<strong>怀疑态度</strong>，这在各模型上都带来了更高的不确定性。</p><p><strong>网页工具即“越狱”。</strong> 我们在实验中保持提示词<strong>一致</strong>，只更换“目标姓名、机构与主张”。有意思的是：<strong>WebSearch 智能体</strong>能为<strong>所有教授</strong>成功生成帖子，但<strong>纯 LLM</strong>出于<strong>安全考虑</strong>对部分教授<strong>拒绝</strong>生成：在主张 1 中，GPT 对一位教授<strong>拒绝</strong>生成；在主张 2 中，共有 <strong>8 位教授</strong>（GPT 1 位、Claude 7 位）被排除。该现象表明：仅仅<strong>启用网页工具</strong>就可能<strong>无意间绕过</strong>这些安全防护，使其在效果上<strong>类似于“越狱”</strong>。关于 LLM 安全防护的更详细讨论见第 7 节。</p><blockquote><p>可以在智能体前先过一遍纯LLM，如果其拒绝生成，则认为此对话有风险？</p></blockquote><h2 id="_6-鱼叉式钓鱼邮件生成" tabindex="-1"><a class="header-anchor" href="#_6-鱼叉式钓鱼邮件生成"><span>6. 鱼叉式钓鱼邮件生成</span></a></h2><p>本节我们考察：在<strong>仅给定一个电子邮箱地址</strong>、<strong>不再需要人工介入</strong>的情况下，LLM 智能体<strong>自动生成个性化钓鱼邮件</strong>的能力。</p><h3 id="_6-1-实验设置" tabindex="-1"><a class="header-anchor" href="#_6-1-实验设置"><span>6.1 实验设置</span></a></h3><p><strong>攻击场景。</strong> 在第 4 节中，我们已证明可以<strong>有效获取</strong>高校研究人员的电子邮箱地址。类似地，攻击者也可滥用<strong>公开可得</strong>的邮箱地址 [43]，或通过<strong>暗网/Telegram 渠道</strong>购买获得 [20]。在现实中，攻击者常用钓鱼邮件诱使目标点击恶意链接或输入个人敏感信息 [10]。在本研究里，攻击者的目标是：利用 LLM 智能体，<strong>围绕受害者邮箱</strong>生成<strong>高度定制</strong>的钓鱼邮件，诱导其点击恶意链接。</p><p>我们的攻击流程旨在制作<strong>具有强欺骗性</strong>的钓鱼邮件。需要强调：攻击者<strong>不进行任何人工操作</strong>，全部由 LLM 智能体完成。该攻击唯一的输入原料是<strong>目标的邮箱地址</strong>，这被加入到 LLM 的提示词里用于生成。</p><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>如图9所示，攻击者仅需向智能体发送<strong>单轮查询</strong>即可生成钓鱼邮件。流程先从<strong>校对日期</strong>开始，以保证邮件时间线与设定情境一致；随后，智能体<strong>围绕目标的邮箱地址</strong>在网上检索个人信息，据此<strong>设计一个逼真的情境</strong>（目标<strong>可能</strong>会因此收到邮件），并确定<strong>可信的发件人身份</strong>。接着，智能体生成一个<strong>看似合理</strong>的 URL 字符串并将其<strong>嵌入</strong>邮件以促进互动。最后，智能体会<strong>伪造发件人邮箱域名</strong>以模仿合法来源，从而<strong>无需</strong>利用官方域的漏洞或<strong>无需</strong>攻陷账号也能发出邮件。</p><p>我们进行了两轮<strong>小规模试验（pilot）</strong>，发现 <strong>WebNav</strong> 与 <strong>WebSearch</strong> 智能体结果相近；出于成本与时间考虑，主实验仅保留 <strong>WebSearch</strong> 智能体生成的邮件用于综合评估。</p><p><strong>用户研究。</strong> 为每位参与者<strong>定制</strong>钓鱼邮件，要求其<strong>亲自</strong>评估这封“发给他/她自己”的邮件。我们据此设计问卷并<strong>主动招募</strong>参与者进行全面评估。</p><p>我们将参与者分为两组：<strong>学术研究人员</strong>与<strong>非学术职场人士</strong>。第 4 节指出，学术研究者的信息在网上<strong>更易公开</strong>，便于攻击者收集；因此我们<strong>假设</strong>他们更易遭遇定向钓鱼并将其纳入研究。同时也引入<strong>非学术职场人士</strong>，以比较<strong>不同组织环境</strong>下的钓鱼易感性，将分析范围扩展至非学术场景。我们将“非学术职场人士”定义为<strong>在非学术机构任职</strong>的个体。</p><p>学术研究人员通过在高校研究社群发布实验招募信息获取（涵盖教授、研究生、本科生等）。非学术职场人士由与作者合作过的业界从业者协助招募。最终我们通过 Google Forms 共招募 <strong>60 名</strong>参与者，并基于其<strong>机构邮箱</strong>进行分层，分析不同机构类型的差异：<strong>55%</strong> 为学术研究人员，<strong>45%</strong> 为非学术职场人士。所有参与者均被告知约 <strong>$7.50</strong> 的报酬。</p><p>需要强调：参与者<strong>并未实际收到</strong>这些邮件；其回答通过 Google 问卷收集。关于采用模拟问卷而非真实投递的理由将在第 8 节讨论。每位参与者评估 <strong>7 封</strong>、以其<strong>自身邮箱</strong>为目标生成的钓鱼邮件；具体邮件类型见下文“Email Design”。</p><p><strong>邮件设计（Email Design）。</strong> 直接使用诸如“phishing email（钓鱼邮件）”等词会触发 LLM 的<strong>安全防护</strong>而被拒绝。因此我们只进行<strong>最低限度的规避</strong>（如避免这些显式术语）。这种最小规避对 <strong>Claude</strong> 与 <strong>GPT</strong> 已足够，但<strong>不足以绕过 Gemini</strong> 的防护，其<strong>拒绝生成</strong>。</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>如表4所示，我们设计了 <strong>7 种</strong>不同类型的邮件，以评估不同变体的效果。邮件目的分为两类：<strong>一般性（general）<strong>与</strong>登录凭证（login credentials）</strong>。</p><ul><li>对于“一般性”目的，令 <strong>WebSearch</strong> 智能体生成一个<strong>现实可信的情境</strong>（目标可能因此收到邮件），并据此撰写邮件；为检验网页搜索功能的价值，我们将其与<strong>纯 LLM</strong>生成的邮件做对比。</li><li>对于“登录凭证”目的，我们给出<strong>更具体的指令</strong>，要求 <strong>WebSearch</strong> 智能体生成<strong>催促目标通过指定链接更新凭证</strong>的邮件。</li><li>另外，为检验<strong>发件人所属机构</strong>的影响，我们修改了 <strong>Claude+WebSearch</strong> 的提示，让<strong>发件人机构</strong>与<strong>收件人机构</strong>不同。</li></ul><p><strong>问卷设计（Questionnaire Design）。</strong> 我们设计了 <strong>7 个问题</strong>，以全面理解参与者对邮件的<strong>感知与互动</strong>（从表层内容到真实性与可能动作）：</p><ul><li>前两问聚焦<strong>内容</strong>：让参与者识别邮件中呈现的信息（Q1），并与其<strong>真实个人信息</strong>对照、判断其准确性（Q2）。</li><li>随后问题关注<strong>角色感知</strong>：收件人是否<strong>识别发件人</strong>（Q3），收件人姓名是否<strong>与其真实姓名匹配</strong>（Q4）。</li><li>接着询问<strong>可能动作</strong>：若收到该邮件，参与者<strong>最可能</strong>做什么（Q5）。</li><li>最后一问评估<strong>感知的真实性</strong>（Q6）：请给出邮件“真实/欺诈”的评分，并指出影响判断的<strong>具体要素</strong>（Q7）。<br> （问卷链接见脚注 5。）</li></ul><h3 id="_6-2-主要结果" tabindex="-1"><a class="header-anchor" href="#_6-2-主要结果"><span>6.2 主要结果</span></a></h3><p>我们首先分析<strong>纯 LLM</strong>与 <strong>WebSearch 智能体</strong>生成的<strong>通用目的（general-purpose）<strong>邮件的有效性与内容；随后基于</strong>参与者分组</strong>对结果进行比较。同时，我们还考察邮件在<strong>与其设定目的</strong>以及<strong>发件人机构</strong>之间的关系下的效果。关于参与者在不同邮件类型下<strong>可能采取的行为</strong>（Q5），综合问卷结果见表7（附录 <strong>E.3</strong>）。</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_6-2-1-总体分析" tabindex="-1"><a class="header-anchor" href="#_6-2-1-总体分析"><span>6.2.1 总体分析</span></a></h4><figure><img src="'+M+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图10 展示了一个由 <strong>Claude+WebSearch</strong> 智能体生成的钓鱼邮件示例。我们仅向智能体提供该知名人物<strong>公开可得的邮箱地址</strong>，模型便据此生成了一封<strong>看似合理</strong>的邮件：包含一个<strong>逼真的 URL</strong>，并提出让 <strong>Meta</strong> 与 <strong>Spotify</strong> 开展合作的建议。</p><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>有效性分析。</strong> 图11(a) 展示了参与者在收到邮件后“<strong>可能采取的行为</strong>”（Q5）的调查结果。我们主要关注<strong>点击链接</strong>的比例，这与攻击者目标直接对应。结果表明：<strong>WebSearch 智能体</strong>相较于<strong>纯 LLM</strong>能促使更多“点击链接”；其中 <strong>Claude+WebSearch</strong> 的<strong>最高点击率</strong>为 <strong>26.67%</strong>。在 <strong>GPT</strong> 的比较中，<strong>WebSearch 智能体</strong>的点击率大约是纯 LLM 的<strong>两倍</strong>。若比较 <strong>Claude</strong> 与 <strong>GPT</strong>，则 <strong>Claude</strong> 效果更好：其<strong>纯 LLM</strong>变体的点击率甚至<strong>略高于</strong> <strong>GPT+WebSearch</strong>。关于 Claude 与 GPT 在<strong>邮件内容</strong>上的差异，下一节将进一步探讨。</p><p>将图11 中的“<strong>行为</strong>”（Q5）与“<strong>感知真实性</strong>”（Q6）对比发现：将邮件判定为“<strong>明确是钓鱼</strong>”的人群，其“<strong>移动到垃圾箱</strong>”的比例与之<strong>高度一致</strong>——平均有 <strong>66.27%</strong> 的受访者在识别为“明确钓鱼”时选择“移至垃圾箱”。但将邮件判定为“<strong>真实</strong>（包含‘明确真实’与‘可能真实’）”的人群比例，与实际“<strong>点击链接</strong>”的人群比例<strong>并不一致</strong>。有趣的是，对于 <strong>WebSearch 智能体</strong>生成的邮件，其<strong>点击率远高于</strong>“被认为真实”的比例——而这一现象在<strong>纯 LLM</strong>生成的邮件中<strong>并不明显</strong>。这种差异主要由被标记为“<strong>不确定</strong>（unsure）”甚至“<strong>钓鱼</strong>（phishing）”的参与者驱动：有 <strong>13.34%</strong> 的人表示，<strong>即使</strong>对 WebSearch 智能体生成的邮件<strong>存疑</strong>，他们仍会点击链接；相比之下，在<strong>纯 LLM</strong>生成的邮件中，这一比例仅为 <strong>3.34%（GPT）<strong>与</strong>10%（Claude）</strong>。表8（附录 <strong>E.3</strong>）给出了一个<strong>列联表</strong>，展示了参与者的<strong>行为（Q5）<strong>与</strong>感知真实性（Q6）<strong>之间的百分比分布。这些结果表明：<strong>WebSearch 智能体</strong>不仅能营造</strong>可信感</strong>，还有效地<strong>利用网络上关于目标的细节信息</strong>来激发收件人的兴趣，从而带来<strong>更高的链接点击率</strong>。</p><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>内容分析。</strong> 我们使用<strong>卡方检验</strong>分析了哪些邮件特征会影响参与者的“<strong>真实性感知</strong>”。结果显示：<strong>邮件内容细节</strong>与<strong>与目标个人信息的一致性</strong>在参与者的真实性判断中起到<strong>显著</strong>作用。卡方检验的统计结果见表6（附录 <strong>E.2</strong>）。</p><figure><img src="'+W+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+E+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在上述统计基础上，我们进一步检验“<strong>能否访问网页</strong>”对这两个因素的影响。图12比较了不同邮件变体中：<strong>具体内容细节的占比</strong>与<strong>这些细节与目标真实信息的一致程度</strong>（基于参与者评估）。比较 <strong>GPT</strong> 与 <strong>Claude</strong> 可知：<strong>GPT</strong> 生成的邮件整体<strong>细节更少</strong>，无论是否使用网页。我们还观察到：<strong>WebSearch 智能体</strong>生成的邮件在<strong>细节丰富度</strong>与<strong>对目标个人数据的准确映射</strong>方面，显著<strong>优于</strong>纯 LLM；这种差异在“<strong>具体专长（specific expertise）</strong>”这一类别中尤为明显。表6（附录 <strong>E.2</strong>）中的卡方检验结果表明：<strong>与目标的具体专长相一致</strong>的内容，显著影响了参与者对邮件真实性的感知。借助来自网络的<strong>与目标专长相关的详细信息</strong>，<strong>WebSearch 智能体</strong>能够生成<strong>更具说服力与实效性</strong>的钓鱼邮件。</p><p><strong>影响“识别钓鱼”判断的因素。</strong> 我们考察了促使参与者将邮件判定为“<strong>钓鱼</strong>”（Q7，多选）的因素，并按影响力排序：<strong>发件人信息（62.3%）</strong>、<strong>邮件目的（39.7%）</strong>、以及<strong>与专长相关的信息（31.6%）<strong>最为显著。这与先前研究一致：用户在评估邮件真实性时，会</strong>高度依赖发件人信息</strong> [61]。这一发现提示：若采用<strong>伪装技术</strong>（例如操纵发件人邮箱），<strong>可能</strong>带来更<strong>有效</strong>的攻击。</p><h4 id="_6-2-2-参与者分组对比" tabindex="-1"><a class="header-anchor" href="#_6-2-2-参与者分组对比"><span>6.2.2 参与者分组对比</span></a></h4><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图13 展示了不同分组的参与者在“收到邮件后将采取哪些行动”的自报告结果。总体而言，<strong>学术研究人员组</strong>相比<strong>非学术职场组</strong>更倾向于<strong>点击链接</strong>。这种差异可能与 Q2（信息准确性验证）中两组“认定邮件包含不准确信息”的比例不同有关：在学术研究人员组中，被判为不准确的邮件比例从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0\\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0%</span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12.1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">12.1\\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">12.1%</span></span></span></span> 不等，说明<strong>大多数邮件包含了准确信息</strong>。而在非学术职场组，即便是 <strong>WebSearch 智能体</strong>生成的邮件，<strong>高达 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40.7</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">40.7\\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">40.7%</span></span></span></span></strong> 的参与者也报告<strong>没有任何信息与自身相符</strong>。这很可能是因为网上关于学术研究人员的可得信息<strong>更为丰富</strong>——研究团队与高校通常会建立网页，公开成员的<strong>在研项目、研究兴趣、最新论文</strong>等信息。</p><p>对于<strong>学术研究人员组</strong>，无论在 <strong>Claude</strong> 还是 <strong>GPT</strong> 下，<strong>WebSearch 智能体</strong>诱导点击的效果都优于<strong>纯 LLM</strong>：点击率分别提升了 <strong>1.37 倍</strong>与<strong>1.75 倍</strong>。在<strong>非学术职场组</strong>中，给 <strong>Claude</strong> 增加网页访问能力后，<strong>链接点击率与纯 LLM 持平</strong>，同时“<strong>移至垃圾箱</strong>”比例<strong>显著下降</strong>。对 <strong>GPT</strong> 而言，增加网页访问能力使<strong>链接点击率提升了 2.49 倍</strong>。这些发现表明：借助网页访问能力，<strong>WebSearch 智能体</strong>能够构造<strong>更有效</strong>的钓鱼邮件；而<strong>研究人员</strong>由于其大量信息在网上公开，尤其<strong>容易受到</strong>此类攻击。</p><h4 id="_6-2-3-提升钓鱼效果的因素" tabindex="-1"><a class="header-anchor" href="#_6-2-3-提升钓鱼效果的因素"><span>6.2.3 提升钓鱼效果的因素</span></a></h4><p>我们进一步考察了<strong>改变邮件目的</strong>与<strong>发件人地址</strong>对钓鱼效果的影响。面向“<strong>登录凭证</strong>”的邮件更能诱导收件人点击链接：<strong>Claude+WebSearch</strong> 的点击率为 <strong>33.33%</strong>，<strong>GPT+WebSearch</strong> 为 <strong>25%</strong>（详见附录 <strong>E.3</strong> 的表7）。这与既有研究一致 [1]：<strong>紧急请求</strong>（如更新登录信息）在钓鱼情境中更有效。</p><p>另一个重要观察是：<strong>发件人域</strong>若来自与目标<strong>不同的机构</strong>，点击率最高，达 <strong>46.67%</strong>，是“通用目的”邮件点击率的 <strong>1.75 倍</strong>（见表7）。参与者对来自他机构的域名<strong>不够熟悉</strong>，更易受骗。</p><p>值得注意的是，我们的结果显示：<strong>WebSearch 智能体</strong>在促使点击方面<strong>可与人工撰写</strong>的鱼叉式钓鱼邮件<strong>同样有效，甚至更有效</strong>。先前研究 [11] 报告：使用内部地址与内部信息的鱼叉式钓鱼，<strong>人工撰写</strong>邮件点击率为 <strong>26.6%</strong>，<strong>LLM 生成</strong>为 <strong>16.7%</strong>。而在我们的研究中，<strong>超过 20%</strong> 的参与者（最高 <strong>46.67%</strong>）表示会点击 <strong>WebSearch 智能体</strong>生成邮件中的链接。虽然由于方法与受试群体不同，上述结果<strong>不可直接对比</strong>，但它们仍表明：<strong>LLM 智能体在生成鱼叉式钓鱼邮件方面非常有效</strong>。尤其值得注意的是，我们的攻击模型<strong>几乎不需要知识与能力积累</strong>、<strong>完全不需要人工撰写</strong>。</p><h2 id="_7-网络攻击的-可接近性-approachability" tabindex="-1"><a class="header-anchor" href="#_7-网络攻击的-可接近性-approachability"><span>7. 网络攻击的“可接近性”（Approachability）</span></a></h2><p>本节我们从两项核心因素评估使用 LLM 智能体实施定向网络攻击的<strong>实用性</strong>：<strong>成本</strong>与<strong>安全防护能力</strong>。理解这两点，有助于判断 LLM 智能体在现实场景中被用于恶意目的的可行性。</p><h3 id="_7-1-成本分析" tabindex="-1"><a class="header-anchor" href="#_7-1-成本分析"><span>7.1 成本分析</span></a></h3><p>实施攻击会产生成本，包括<strong>时间成本</strong>与<strong>金钱成本</strong>，这将显著影响利用 LLM 智能体开展攻击的实用性。我们衡量攻击者在<strong>商用可得 LLM</strong>下，<strong>多快</strong>、<strong>以何种成本</strong>可以完成攻击。</p><p>我们记录了在各个攻击情景中，针对每个目标、使用 <strong>WebSearch 智能体</strong>时所消耗的<strong>时间</strong>与<strong>token 数</strong>。为简化计算，我们用“<strong>输入 token + 输出 token</strong>”之和近似“<strong>总 token</strong>”，并按<strong>输出单价</strong>计费（通常高于输入单价），以此估计 <strong>API 成本上界</strong>。各模型的参考定价：<strong>GPT-4o</strong> 与 <strong>Claude 3.5 Sonnet</strong> 为 <strong>$15 / 100 万 tokens</strong>；<strong>Gemini 1.5 Flash</strong> 为 <strong>$0.30 / 100 万 tokens</strong>。token 计数方面：GPT 使用 <strong>tiktoken</strong> 分词器 [50]；Claude 与 Gemini 使用其 API 提供的<strong>token 计数</strong>功能 [5,16]。</p><p><strong>平均结果如下：</strong></p><ul><li><strong>PII 收集</strong>：单个体平均 <strong>9.1 s</strong>，成本 <strong>$0.022</strong>（GPT：<strong>6.7 s</strong>, <strong>$0.021</strong>；Claude：<strong>11.4 s</strong>, <strong>$0.022</strong>）。</li><li><strong>冒充帖生成</strong>：GPT、Claude、Gemini 分别平均 <strong>9.4 s、21.7 s、3.6 s</strong>，成本均 <strong>&lt; $0.03</strong>（GPT：<strong>$0.022</strong>；Claude：<strong>$0.030</strong>；Gemini：<strong>$0.0003</strong>）。</li><li><strong>钓鱼邮件生成</strong>：GPT、Claude 分别平均 <strong>7.1 s、22.7 s</strong>，成本均 <strong>&lt; $0.04</strong>（GPT：<strong>$0.027</strong>；Claude：<strong>$0.039</strong>）。成本统计详见<strong>附录 D</strong>。</li></ul><p>不同 LLM 的成本差异可归因于<strong>模型能力差异</strong>（见 [2] 的分析）。总体看，<strong>WebSearch 智能体</strong>高度<strong>实用</strong>：能以<strong>极低成本</strong>、<strong>高速</strong>完成任务。借助 WebSearch，攻击者可以在网络攻击中<strong>快速且低成本</strong>地利用私人数据，凸显了其被恶意滥用的<strong>现实可行性</strong>。</p><h3 id="_7-2-安全防护能力-safeguard-capability" tabindex="-1"><a class="header-anchor" href="#_7-2-安全防护能力-safeguard-capability"><span>7.2 安全防护能力（Safeguard Capability）</span></a></h3><p>我们通过考察<strong>冒充帖生成</strong>与<strong>鱼叉式钓鱼邮件生成</strong>中的不同因素，对各模型的安全防护能力进行了深入分析。</p><p><strong>冒充帖生成。</strong> 在第 5 节我们发现，安全防护是否触发取决于<strong>主张（claim）<strong>的性质。为进一步分析各模型的防护能力，我们又加入了两个主张：“<strong>投资 Dogecoin</strong>”与“<strong>LLM 在生成钓鱼邮件方面很有效</strong>”，并在</strong>不同目标群体</strong>上测试其绕过防护的情况。两个目标群体分别为：先前分析中使用的 <strong>10 位教授</strong>，以及<strong>科技行业中 10 位影响力人物</strong>（更知名、公众关注度更高），作为被冒充的对象。</p><p>我们观察到，防护的触发不仅受<strong>主张性质</strong>影响，也受<strong>被冒充对象</strong>的性质影响。在这项任务中，各服务的防护强度不同：<strong>Claude</strong> 最严格，<strong>GPT</strong> 对提示最宽松。对 <strong>GPT</strong> 而言，<strong>纯 LLM</strong>与<strong>WebSearch 智能体</strong>在这两个新增主张上都<strong>未发生拒绝</strong>，均能成功生成内容。而 <strong>Claude</strong> 与 <strong>Gemini</strong> 的结果不同——当主张是“投资 Dogecoin”时，<strong>WebSearch 智能体</strong>可以生成帖子，但对应的<strong>纯 LLM</strong>只愿意为<strong>教授</strong>生成内容，并分别以 <strong>60%</strong> 与 <strong>20%</strong> 的比例<strong>拒绝</strong>为<strong>影响力人物</strong>生成内容。对于“<strong>LLM 的有害用途</strong>”这一主张，<strong>Gemini 的 WebSearch 智能体与纯 LLM</strong>都能成功生成内容；<strong>Claude</strong> 则直接拒绝，回复：“<strong>我不会创作宣扬 AI 系统有害用途的内容</strong>”。这种<strong>对目标对象的依赖性</strong>会限制攻击者可选择的目标范围。</p><p><strong>鱼叉式钓鱼邮件生成。</strong> 我们从两个关键因素评估各模型在钓鱼邮件生成方面的防护能力：1）<strong>邮件目的</strong>（通用、索要登录凭证、索要金钱）；2）<strong>邮箱类型</strong>（机构邮箱 vs. Gmail）。我们使用作者本人在知情同意下的邮箱地址来生成邮件。</p><p><strong>Gemini</strong> 的防护最强：在<strong>纯 LLM</strong>与<strong>WebSearch 智能体</strong>的所有场景中都<strong>拒绝生成</strong>邮件，并返回类似“<strong>我无法满足你的请求。未经同意生成邮件既不道德也可能违法</strong>”的消息。所有<strong>不具备网页访问能力</strong>的模型在“<strong>索要登录凭证</strong>或<strong>索要金钱</strong>”的目的下都会拒绝生成，说明这些主题会触发更严格的防护机制；相反，<strong>通用目的</strong>的邮件更不容易被拦截，<strong>GPT</strong> 与 <strong>Claude</strong> 在此类场景下会生成回复。相较于 <strong>Gmail 地址</strong>，当提供<strong>机构邮箱</strong>时，<strong>GPT</strong> 与 <strong>Claude</strong> 更可能生成邮件。</p><p>有趣的是，一旦<strong>加入网页访问能力</strong>：此前在严格设置下<strong>拒绝生成</strong>内容的 <strong>WebSearch 智能体</strong>开始<strong>产出</strong>回复；网页工具提升了 <strong>GPT</strong> 与 <strong>Claude</strong> 的<strong>绕过防护</strong>能力，使其在我们多数实验设置中都能生成邮件。这表明：<strong>基于网页的工具可能为 LLM 带来“越狱”式的脆弱性</strong>。</p><h2 id="_8-讨论与局限" tabindex="-1"><a class="header-anchor" href="#_8-讨论与局限"><span>8. 讨论与局限</span></a></h2><h3 id="_8-1-采用问卷调查的形式" tabindex="-1"><a class="header-anchor" href="#_8-1-采用问卷调查的形式"><span>8.1 采用问卷调查的形式</span></a></h3><p>在本研究中，我们选择了基于问卷的方式来理解用户对钓鱼邮件的行为，原因如下。</p><p><strong>理解意图与推理。</strong> 基于问卷的研究使我们能够探究参与者行动背后的理由与意图，提供仅靠点击率分析难以捕捉的洞见。先前研究也认可这种方法是理解安全情境下用户行为的有效手段（见文献 [31, 39, 61]）。</p><p><strong>应对伦理顾虑。</strong> 模拟钓鱼攻击往往涉及具有欺骗性的操作，可能引发重大伦理问题（见文献 [13, 17, 59]）。此前使用仿真的研究通常局限于在具备全面控制与机构审查条件的单一组织内进行（见文献 [11, 37]）。这类受控环境虽然有助于管理伦理风险，但会限制参与者群体的多样性。通过采用问卷的方式，我们得以在不触碰伦理风险的前提下，从 25 家不同的组织收集到有价值的数据。</p><h3 id="_8-2-防御" tabindex="-1"><a class="header-anchor" href="#_8-2-防御"><span>8.2 防御</span></a></h3><p>我们提出了一些针对利用基于网络工具收集个人可识别信息（PII）的 LLM 所导致的未授权收集行为的防御策略。首先，<strong>LLM 服务提供商</strong>可以制定规则，要求使用网页抓取工具的 LLM 在访问网站前检查并遵守网站 <em>robots.txt</em> 文件中的指令。该文件是国际上通用的建议规范，用于允许或限制网络爬虫收集站点与页面数据。遵循这些指令可以防止 LLM 无意或恶意地从网站受限区域抓取敏感信息。此外，还可以部署可扩展的安全保护措施，详见第 8.3 节的进一步讨论。</p><p>其次，<strong>PII 提供方</strong>（如网站管理者）在处理敏感信息时应主动进行控制，减少其在线暴露。一个直接的做法是编写并定期更新健壮的 <em>robots.txt</em> 文件，明确阻止爬虫访问包含个人信息的页面。除此之外，还可采用更具创造性的策略来“误导”LLM 收集到不正确或不完整的 PII。举例来说，网站可以向自动化爬虫展示被扰乱或虚假的个人数据，而只有在用户执行某个明确动作后才显示正确信息。这样，人类访客能够看到准确的细节，而进行自动抓取的 LLM 则更可能无意中收集到错误信息，从而降低未授权访问或滥用的风险。</p><h3 id="_8-3-能力扩张带来的风险" tabindex="-1"><a class="header-anchor" href="#_8-3-能力扩张带来的风险"><span>8.3 能力扩张带来的风险</span></a></h3><p>LLM 的发展不仅体现在其执行复杂任务的熟练程度不断提升，同时也凸显了其在网络攻击等场景中的潜在滥用风险。我们的研究发现表明，当 LLM 被赋予更多工具时，其在网络攻击中的有效性会显著增强。能力的增强与风险的上升呈相关关系，这强调了需要与之相称、并随之增强的安全防护。</p><p>为应对这些不断增长的担忧，一些公司（例如 Anthropic）已经开发了可随风险规模扩展的安全防护机制（见文献 [3]）。此类方法通常会设定阈值：当 AI 系统的能力达到某一水平时，就必须启用更严格的安全措施。一旦达到阈值，系统会自动实施更强的安全控制，以匹配该 AI 能力所带来的风险等级。随着 LLM 能力的持续提高，实施具备可扩展性和自适应性的安全措施将至关重要，以确保其安全且合乎伦理地被使用。</p><h3 id="_8-4-研究局限" tabindex="-1"><a class="header-anchor" href="#_8-4-研究局限"><span>8.4 研究局限</span></a></h3><p>在第 5 节中，我们的主要目标之一是评估不同能力水平的 LLM 代理在生成冒充帖（impersonation posts）方面的有效性。要理解工具使用与冒充效果之间的联系，理想状态是能在统一条件下进行实验，例如使用一致的目标主张。但从我们的目标对象——计算机科学教授——那里获取针对特定主张的真实帖子在实践中不可行。因此，我们无法使用真实样本进行对照。尽管如此，这一限制并未削弱我们研究的主要目标。</p><p>我们也依赖 LLM 评审者来判断冒充帖子。虽然这些评审者与人类审稿人的判断结果具有较高一致性，但它们可能仍然不足以完全捕捉到人类专家所具备的细致推理和情境理解。</p><p>在第 6 节中，我们重点分析了参与者与 LLM 生成的钓鱼邮件交互的方式。尽管将合成的非钓鱼邮件纳入研究可以提供有价值的基线用于对比，但当前研究并未将其纳入。我们建议将其作为未来研究的一个有前景的方向，以更好地理解用户对真实邮件与钓鱼邮件的响应差异。</p><h2 id="_9-结论" tabindex="-1"><a class="header-anchor" href="#_9-结论"><span>9. 结论</span></a></h2><p>在本文中，我们探讨了 LLM 代理在网络攻击中的新兴威胁，尤其是它们在利用基于网络的工具来发动涉及私人数据的攻击时的潜在用法。我们重点关注了三类网络攻击：PII 收集、冒充帖生成以及鱼叉式钓鱼邮件生成。实验结果显示，攻击者可以成功地使用 LLM 代理自动化这些攻击，而基于网络的工具会进一步提升这些攻击的表现。此外，我们的结果还表明，现有的 LLM 安全防护可以被以低成本轻易绕过，使得在网络攻击中使用 LLM 代理成为一个可行选项。上述发现暴露出当前防护中的显著薄弱环节，并突显出迫切需要采取更强有力的安全措施，以防止 LLM 代理被滥用。</p>',174)]))}const G=n(I,[["render",P],["__file","Emerging_Threat_of_Web-Enabled_LLMs.html.vue"]]),C=JSON.parse(`{"path":"/posts/scholar/Emerging_Threat_of_Web-Enabled_LLMs.html","title":"When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs","lang":"zh-CN","frontmatter":{"icon":"pen-to-square","date":"2025-11-06T00:00:00.000Z","category":["AI Security"],"description":"When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs 论文来自 34th USENIX Security Symposium (SEC 25) 的《When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs》。 摘要 近年来...","head":[["meta",{"property":"og:url","content":"https://iXanadu13.github.io/posts/scholar/Emerging_Threat_of_Web-Enabled_LLMs.html"}],["meta",{"property":"og:site_name","content":"Xanadu13's Blog"}],["meta",{"property":"og:title","content":"When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"}],["meta",{"property":"og:description","content":"When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs 论文来自 34th USENIX Security Symposium (SEC 25) 的《When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs》。 摘要 近年来..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-11-06T13:06:20.000Z"}],["meta",{"property":"article:published_time","content":"2025-11-06T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-11-06T13:06:20.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs\\",\\"image\\":[\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure1.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure2.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table1.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure3.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure4.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure5.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure6.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure7.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure8.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table2.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table3.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure9.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table4.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table7.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure10.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure11.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table8.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/table6.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure12.png\\",\\"https://iXanadu13.github.io/assets/images/scholar/Emerging_Threat_of_Web-Enabled_LLMs/figure13.png\\"],\\"datePublished\\":\\"2025-11-06T00:00:00.000Z\\",\\"dateModified\\":\\"2025-11-06T13:06:20.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Xanadu13\\",\\"url\\":\\"https://github.com/iXanadu13\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"1. 引言","slug":"_1-引言","link":"#_1-引言","children":[{"level":3,"title":"1.1 我们的工作","slug":"_1-1-我们的工作","link":"#_1-1-我们的工作","children":[]},{"level":3,"title":"1.2 研究问题","slug":"_1-2-研究问题","link":"#_1-2-研究问题","children":[]},{"level":3,"title":"1.3 贡献","slug":"_1-3-贡献","link":"#_1-3-贡献","children":[]}]},{"level":2,"title":"2. 相关工作","slug":"_2-相关工作","link":"#_2-相关工作","children":[{"level":3,"title":"LLM 的安全性","slug":"llm-的安全性","link":"#llm-的安全性","children":[]},{"level":3,"title":"面向网络攻击的 LLM 智能体","slug":"面向网络攻击的-llm-智能体","link":"#面向网络攻击的-llm-智能体","children":[]}]},{"level":2,"title":"3. 用于网络攻击的 LLM 智能体","slug":"_3-用于网络攻击的-llm-智能体","link":"#_3-用于网络攻击的-llm-智能体","children":[{"level":3,"title":"3.1 LLM 智能体概述","slug":"_3-1-llm-智能体概述","link":"#_3-1-llm-智能体概述","children":[]},{"level":3,"title":"3.2 定向网络攻击","slug":"_3-2-定向网络攻击","link":"#_3-2-定向网络攻击","children":[]}]},{"level":2,"title":"4. PII 收集","slug":"_4-pii-收集","link":"#_4-pii-收集","children":[{"level":3,"title":"4.1 实验设置","slug":"_4-1-实验设置","link":"#_4-1-实验设置","children":[]},{"level":3,"title":"4.2 主要结果","slug":"_4-2-主要结果","link":"#_4-2-主要结果","children":[]},{"level":3,"title":"4.3 模型分析","slug":"_4-3-模型分析","link":"#_4-3-模型分析","children":[]}]},{"level":2,"title":"5. 冒充帖（Impersonation Post）生成","slug":"_5-冒充帖-impersonation-post-生成","link":"#_5-冒充帖-impersonation-post-生成","children":[{"level":3,"title":"5.1 实验设置","slug":"_5-1-实验设置","link":"#_5-1-实验设置","children":[]},{"level":3,"title":"5.2 主要结果","slug":"_5-2-主要结果","link":"#_5-2-主要结果","children":[]}]},{"level":2,"title":"6. 鱼叉式钓鱼邮件生成","slug":"_6-鱼叉式钓鱼邮件生成","link":"#_6-鱼叉式钓鱼邮件生成","children":[{"level":3,"title":"6.1 实验设置","slug":"_6-1-实验设置","link":"#_6-1-实验设置","children":[]},{"level":3,"title":"6.2 主要结果","slug":"_6-2-主要结果","link":"#_6-2-主要结果","children":[]}]},{"level":2,"title":"7. 网络攻击的“可接近性”（Approachability）","slug":"_7-网络攻击的-可接近性-approachability","link":"#_7-网络攻击的-可接近性-approachability","children":[{"level":3,"title":"7.1 成本分析","slug":"_7-1-成本分析","link":"#_7-1-成本分析","children":[]},{"level":3,"title":"7.2 安全防护能力（Safeguard Capability）","slug":"_7-2-安全防护能力-safeguard-capability","link":"#_7-2-安全防护能力-safeguard-capability","children":[]}]},{"level":2,"title":"8. 讨论与局限","slug":"_8-讨论与局限","link":"#_8-讨论与局限","children":[{"level":3,"title":"8.1 采用问卷调查的形式","slug":"_8-1-采用问卷调查的形式","link":"#_8-1-采用问卷调查的形式","children":[]},{"level":3,"title":"8.2 防御","slug":"_8-2-防御","link":"#_8-2-防御","children":[]},{"level":3,"title":"8.3 能力扩张带来的风险","slug":"_8-3-能力扩张带来的风险","link":"#_8-3-能力扩张带来的风险","children":[]},{"level":3,"title":"8.4 研究局限","slug":"_8-4-研究局限","link":"#_8-4-研究局限","children":[]}]},{"level":2,"title":"9. 结论","slug":"_9-结论","link":"#_9-结论","children":[]}],"git":{"createdTime":1762434380000,"updatedTime":1762434380000,"contributors":[{"name":"Xanadu13","email":"xanadu13@qq.com","commits":1}]},"readingTime":{"minutes":48.03,"words":14408},"filePathRelative":"posts/scholar/Emerging_Threat_of_Web-Enabled_LLMs.md","localizedDate":"2025年11月6日","excerpt":"\\n<p>论文来自 34th USENIX Security Symposium (SEC 25)\\n的《<a href=\\"https://www.usenix.org/conference/usenixsecurity25/presentation/kim-hanna\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</a>》。</p>\\n<h2>摘要</h2>\\n<p>近年来，大型语言模型（LLMs）的快速发展使其逐渐成为具备规划能力并可与各类工具交互的“智能体”系统。这些 LLM 智能体常与基于网页的工具配合使用，从而能够访问多样化的信息源与实时数据。尽管这些进步在诸多应用中带来了显著益处，它们也同时增加了被恶意滥用的风险，尤其是在涉及个人信息的网络攻击情境中。</p>","autoDesc":true}`);export{G as comp,C as data};
